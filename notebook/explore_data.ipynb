{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "figure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import ppscore as pps\n",
    "import preprocessing_census\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file_path = \"../input/us_census_full/labels.txt\"\n",
    "with open(label_file_path) as f:\n",
    "    names = [name.strip().replace(' ', '_') for name in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../input/us_census_full/census_income_learn.csv\"\n",
    "test_path = \"../input/us_census_full/census_income_test.csv\"\n",
    "train = pd.read_csv(train_path, names=names, sep=\", \")\n",
    "test = pd.read_csv(test_path, names=names, sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data size : \" + str(train.shape))\n",
    "print(\"Test data size : \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"income\"].loc[(train[\"income\"] == \"50000+.\")] = 1\n",
    "train[\"income\"].loc[(train[\"income\"] == \"- 50000.\")] = 0\n",
    "test[\"income\"].loc[(test[\"income\"] == \"50000+.\")] = 1\n",
    "test[\"income\"].loc[(test[\"income\"] == \"- 50000.\")] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"income\"\n",
    "ratio_count = pd.DataFrame(train[col].value_counts(dropna=False))\n",
    "ratio_count[\"ratio\"] = ratio_count/ratio_count.sum()\n",
    "ratio_count[\"counts\"] = ratio_count[col]\n",
    "ratio_count[col] = ratio_count.index\n",
    "ratio_count.sort_values('ratio',ascending=False,inplace=True)\n",
    "fig = px.bar(ratio_count, x=col, y='counts')\n",
    "fig.show()\n",
    "display(ratio_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy need to be higher than 0.937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing objects type data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objects_ratios(df):\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        if dtype == \"object\":\n",
    "            print(col)\n",
    "            ratio_count = pd.DataFrame(train[col].value_counts(dropna=False))\n",
    "            ratio_count[\"ratio\"] = ratio_count/ratio_count.sum()\n",
    "            ratio_count[\"counts\"] = ratio_count[col]\n",
    "            ratio_count[col] = ratio_count.index\n",
    "            ratio_count.sort_values('ratio',ascending=False,inplace=True)\n",
    "            fig = px.bar(ratio_count, x=col, y='counts')\n",
    "            fig.show()\n",
    "            display(ratio_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_of_worker -> categorical<br/>\n",
    "education -> ordinal?<br/>\n",
    "enroll_in_edu_inst_last_wk -> categorical<br/>\n",
    "marital_stat -> categorical<br/>\n",
    "major_industry_code -> categorical\n",
    "major_occupation_code -> categorical<br/>\n",
    "race -> categorical<br/>\n",
    "hispanic_origin -> categorical<br/>\n",
    "sex -> categorical<br/>\n",
    "member_of_a_labor_union -> categorical<br/>\n",
    "reason_for_unemployment -> categorical<br/>\n",
    "full_or_part_time_employment_stat -> categorical<br/>\n",
    "tax_filer_status -> categorical<br/>\n",
    "region_of_previous_residence -> categorical<br/>\n",
    "state_of_previous_residence -> categorical, **Not in universe & ?**<br/>\n",
    "detailed_household_and_family_stat -> categorical<br/>\n",
    "migration_code-change_in_msa -> categorical, **Not in universe & Not identifiable & ?**<br/>\n",
    "migration_code-change_in_reg -> categorical, **Not in universe & ?**<br/>\n",
    "migration_code-move_within_reg -> categorical, **Not in universe & ?**<br/>\n",
    "live_in_this_house_1_year_ago -> categorical<br/>\n",
    "migration_prev_res_in_sunbelt -> categorical, **Not in universe & ?**<br/>\n",
    "family_members_under_18 -> categorical<br/>\n",
    "country_of_birth_father -> categorical<br/>\n",
    "country_of_birth_mother -> categorical<br/>\n",
    "country_of_birth_self -> categorical<br/>\n",
    "citizenship -> categorical<br/>\n",
    "fill_inc_questionnaire_for_veteran's_admin -> categorical<br/>\n",
    "income -> categorical<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_objects_ratios(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_objects_ratios(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def missing_data(df):\n",
    "    # replace 'Not in universe', 'Not identifiable' and '?' by null\n",
    "    df.replace(\"Not in universe\", np.nan, inplace=True)\n",
    "    df.replace(\"Not identifiable\", np.nan, inplace=True)\n",
    "    df.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "    total = df.isnull().sum()\n",
    "    percent = (df.isnull().sum()/df.shape[0]*100)\n",
    "    missing_values = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        types.append(dtype)\n",
    "    missing_values[\"Types\"] = types\n",
    "    missing_values.sort_values('Total',ascending=False,inplace=True)\n",
    "    return(missing_values)\n",
    "missing_datas = missing_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns with too much missing data:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cols = missing_datas[missing_datas[\"Percent\"] >= 51].index\n",
    "print(empty_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select_dtypes(exclude=[\"object\"]).hist(bins=50, figsize=(20,15))\n",
    "plt.tight_layout(pad=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(train.corr(), cmap=\"Blues\", linewidths=0.75, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_bins(col):\n",
    "    df_bins = train.copy()\n",
    "    df_bins[\"bins\"] = pd.cut(train[col], bins=5)\n",
    "    plt.subplots(figsize=(10,10))\n",
    "    sns.countplot(\"bins\",hue='income',data=df_bins, palette='RdBu_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bins(\"detailed_industry_recode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bins(\"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sup = train[train[\"income\"] == 1]\n",
    "train_inf = train[train[\"income\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_comparison(col):\n",
    "    train_sup_col = train_sup[col].value_counts()\n",
    "    train_sup_col = pd.DataFrame({col:train_sup_col.index, 'count':train_sup_col.values})\n",
    "\n",
    "    train_inf_col = train_inf[col].value_counts()\n",
    "    train_inf_col = pd.DataFrame({col:train_inf_col.index, 'count':train_inf_col.values})\n",
    "\n",
    "    pie_sup = go.Pie(  \n",
    "       labels = train_sup_col[col],\n",
    "       values = train_sup_col[\"count\"],\n",
    "       domain=dict(x=[0, 0.5]),\n",
    "       name=\"Above 50k\",\n",
    "       hole = 0.5,\n",
    "       marker = dict(colors=['violet', 'cornflowerblue'], line=dict(color='#000000', width=2))\n",
    "    )\n",
    "\n",
    "    pie_inf = go.Pie(  \n",
    "       labels = train_inf_col[col],\n",
    "       values = train_inf_col[\"count\"],\n",
    "       domain=dict(x=[0.5, 1.0]), \n",
    "       name=\"Below 50k\",\n",
    "       hole = 0.5,\n",
    "       marker = dict(colors=['cornflowerblue', 'violet'], line=dict(color='#000000', width=2))\n",
    "    )\n",
    "\n",
    "    data = [pie_sup, pie_inf]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        title=col + ' percentage from +50k vs -50k',\n",
    "        annotations=[dict(text=\"Above 50k\", x=0.18, y=0.5, font_size=15, showarrow=False),\n",
    "                     dict(text=\"Below 50k\", x=0.85, y=0.5, font_size=15, showarrow=False)]\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_comparison(\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_comparison(\"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, target, categorical_columns, numeric_columns = preprocessing_census.feature_transform(train, empty_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek('auto')\n",
    "X_smt, y_smt = smt.fit_sample(df, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(target).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_objects_ratios(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, make_scorer\n",
    "\n",
    "# Function responsible for checking our model's performance on the test data\n",
    "def testSetResultsClassifier(classifier, x_test, y_test):\n",
    "    predictions = classifier.predict(x_test)\n",
    "    \n",
    "    results = []\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    results.append(f1)\n",
    "    results.append(precision)\n",
    "    results.append(recall)\n",
    "    results.append(roc_auc)\n",
    "    results.append(accuracy)\n",
    "    \n",
    "    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n",
    "    print(\"F1 score, Precision, Recall, ROC_AUC score, Accuracy:\")\n",
    "    print(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineBestModelPipeline(df, target, categorical_columns, numeric_columns):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.10, random_state=42)\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    # # Obtaining balanced data for modeling using Random Under Sampling\n",
    "    #x_train, y_train = balancingClassesRus(x_train, y_train)\n",
    "\n",
    "    # # Obtaining balanced data for modeling using SMOTEENN\n",
    "    #x_train, y_train = balancingClassesSmoteenn(x_train, y_train)\n",
    "\n",
    "    # # Obtaining balanced data for modeling using SMOTE\n",
    "    #x_train, y_train = balancingClassesSmote(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    #Numeric Transformers\n",
    "    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),\n",
    "                                            ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),\n",
    "                                            ('scaler', StandardScaler())])\n",
    "    \n",
    "    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n",
    "                                            ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n",
    "                                            ('scaler', StandardScaler())])\n",
    "    \n",
    "    \n",
    "    #Categorical Transformer\n",
    "    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n",
    "                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n",
    "    \n",
    "    \n",
    "    #Combining both numerical and categorical pipelines\n",
    "    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    \n",
    "    # Define the Pipeline\n",
    "    pipe = Pipeline(steps=[('data_transformations', data_transformations_1),\n",
    "                           ('feature_eng', PCA()),\n",
    "                           ('clf', SVC())])\n",
    "                           #memory='cache_folder')\n",
    "    \n",
    "    \n",
    "    # Use RandomSearchCV\n",
    "    params_grid = [\n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [KNeighborsClassifier()],\n",
    "#                      'clf__n_neighbors': stats.randint(1, 50),\n",
    "#                      'clf__metric': ['minkowski', 'euclidean']},\n",
    "\n",
    "        \n",
    "\n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [LogisticRegression()],\n",
    "#                      'clf__penalty': ['l1', 'l2'],\n",
    "#                      'clf__C': stats.uniform(0.01, 10)},\n",
    "\n",
    "\n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [SVC()],\n",
    "#                      'clf__C': stats.uniform(0.1, 10),\n",
    "#                      'clf__gamma': stats.uniform(0.1, 10)},\n",
    "\n",
    "\n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [DecisionTreeClassifier()],\n",
    "#                      'clf__criterion': ['gini', 'entropy'],\n",
    "#                      'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "#                      'clf__max_depth': [None, stats.randint(1, 5)]},\n",
    "\n",
    "\n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [RandomForestClassifier()],\n",
    "#                      'clf__n_estimators': stats.randint(10, 175),\n",
    "#                      'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "#                      'clf__max_depth': [None, stats.randint(1, 5)],\n",
    "#                      'clf__random_state': stats.randint(1, 49)},\n",
    "        \n",
    "                    \n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [ExtraTreesClassifier()],\n",
    "#                      'clf__n_estimators': stats.randint(10, 150),\n",
    "#                      'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "#                      'clf__max_depth': [None, stats.randint(1, 6)]},\n",
    "\n",
    "                    \n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [GradientBoostingClassifier()],\n",
    "#                      'clf__n_estimators': stats.randint(10, 100),\n",
    "#                      'clf__learning_rate': stats.uniform(0.01, 0.7),\n",
    "#                      'clf__max_depth': [None, stats.randint(1, 6)]},\n",
    "\n",
    "        \n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [LGBMClassifier()],\n",
    "#                      'clf__n_estimators': stats.randint(1, 100),\n",
    "#                      'clf__learning_rate': stats.uniform(0.01, 0.7),\n",
    "#                      'clf__max_depth': [None, stats.randint(1, 6)]},\n",
    "\n",
    "\n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [XGBClassifier()],\n",
    "#                      'clf__n_estimators': stats.randint(5, 125),\n",
    "#                      'clf__eta': stats.uniform(0.01, 1),\n",
    "#                      'clf__max_depth': [None, stats.randint(1, 6)],\n",
    "#                      'clf__gamma': stats.uniform(0.01, 1)},\n",
    "\n",
    "\n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1\n",
    "                                              #,data_transformations_2, data_transformations_3, data_transformations_4\n",
    "                                             ],\n",
    "                     'feature_eng': [#None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2),\n",
    "                         PolynomialFeatures(degree=3)],\n",
    "                     'clf': [StackingClassifier(estimators=[('svc', SVC(C=1, gamma=1)),\n",
    "                                                            ('rf', RandomForestClassifier(max_depth=5, n_estimators=50, n_jobs=-1, random_state=28)),\n",
    "                                                            ('xgb', XGBClassifier(eta=0.6, gamma=0.7, max_depth=None, n_estimators=30))], \n",
    "                                                final_estimator=LogisticRegression(C=1))]}\n",
    "   \n",
    "   \n",
    "        \n",
    "#                     {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "#                      'feature_eng': [None, \n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "#                                      PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "#                                      PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "#                      'clf': [VotingClassifier(estimators=[('gbt', GradientBoostingClassifier(learning_rate=0.8, max_depth=None, n_estimators=30)),\n",
    "#                                                           ('lgbm', LGBMClassifier(n_estimators=30, learning_rate=0.6, max_depth=None)),\n",
    "#                                                           ('xgb', XGBClassifier(eta=0.8, gamma=0.8, max_depth=None, n_estimators=40))],\n",
    "#                                               voting='soft')]}\n",
    "                ]\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    cross_validator = StratifiedShuffleSplit(n_splits=10, train_size=0.8, test_size=0.2, random_state=7)\n",
    "    \n",
    "    best_model_pipeline = RandomizedSearchCV(estimator=pipe, param_distributions=params_grid, \n",
    "                                             n_iter=100, scoring=metrics, refit='accuracy', \n",
    "                                             n_jobs=-1, cv=cross_validator, random_state=21)\n",
    "\n",
    "    best_model_pipeline.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    # Results\n",
    "    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n",
    "    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n",
    "    print(\"\\n\\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n",
    "    print(\"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, best_model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(df, target, categorical_columns, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_results = testSetResultsClassifier(best_model_pipeline, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(best_model_pipeline.cv_results_)\n",
    "\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_results[df_results['rank_test_accuracy'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = target\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    \n",
    "    print(alg)\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1, target, cv  = cv_split)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    #MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1, target)\n",
    "    MLA_predict[MLA_name] = alg.predict(data1)\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, target, categorical_columns, numeric_columns = feature_transform(train, empty_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = best_model_pipeline.predict(df_test)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
