{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "figure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import ppscore as pps\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "#KFold, StratifiedKFold, StratifiedShuffleSplit, RandomizedSearchCV, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file_path = \"../input/us_census_full/labels.txt\"\n",
    "with open(label_file_path) as f:\n",
    "    names = [name.strip().replace(' ', '_') for name in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../input/us_census_full/census_income_learn.csv\"\n",
    "test_path = \"../input/us_census_full/census_income_test.csv\"\n",
    "train = pd.read_csv(train_path, names=names, sep=\", \")\n",
    "test = pd.read_csv(test_path, names=names, sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data size : \" + str(train.shape))\n",
    "print(\"Test data size : \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"income\"].loc[(train[\"income\"] == \"50000+.\")] = 1\n",
    "train[\"income\"].loc[(train[\"income\"] == \"- 50000.\")] = 0\n",
    "test[\"income\"].loc[(test[\"income\"] == \"50000+.\")] = 1\n",
    "test[\"income\"].loc[(test[\"income\"] == \"- 50000.\")] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"income\"\n",
    "ratio_count = pd.DataFrame(train[col].value_counts(dropna=False))\n",
    "ratio_count[\"ratio\"] = ratio_count/ratio_count.sum()\n",
    "ratio_count[\"counts\"] = ratio_count[col]\n",
    "ratio_count[col] = ratio_count.index\n",
    "ratio_count.sort_values('ratio',ascending=False,inplace=True)\n",
    "fig = px.bar(ratio_count, x=col, y='counts')\n",
    "fig.show()\n",
    "display(ratio_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying objects type data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objects_ratios(df):\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        if dtype == \"object\":\n",
    "            print(col)\n",
    "            ratio_count = pd.DataFrame(train[col].value_counts(dropna=False))\n",
    "            ratio_count[\"ratio\"] = ratio_count/ratio_count.sum()\n",
    "            ratio_count[\"counts\"] = ratio_count[col]\n",
    "            ratio_count[col] = ratio_count.index\n",
    "            ratio_count.sort_values('ratio',ascending=False,inplace=True)\n",
    "            fig = px.bar(ratio_count, x=col, y='counts')\n",
    "            fig.show()\n",
    "            display(ratio_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_of_worker -> categorical<br/>\n",
    "education -> ordinal?<br/>\n",
    "enroll_in_edu_inst_last_wk -> categorical<br/>\n",
    "marital_stat -> categorical<br/>\n",
    "major_industry_code -> categorical\n",
    "major_occupation_code -> categorical<br/>\n",
    "race -> categorical<br/>\n",
    "hispanic_origin -> categorical<br/>\n",
    "sex -> categorical<br/>\n",
    "member_of_a_labor_union -> categorical<br/>\n",
    "reason_for_unemployment -> categorical<br/>\n",
    "full_or_part_time_employment_stat -> categorical<br/>\n",
    "tax_filer_status -> categorical<br/>\n",
    "region_of_previous_residence -> categorical<br/>\n",
    "state_of_previous_residence -> categorical, **Not in universe & ?**<br/>\n",
    "detailed_household_and_family_stat -> categorical<br/>\n",
    "migration_code-change_in_msa -> categorical, **Not in universe & Not identifiable & ?**<br/>\n",
    "migration_code-change_in_reg -> categorical, **Not in universe & ?**<br/>\n",
    "migration_code-move_within_reg -> categorical, **Not in universe & ?**<br/>\n",
    "live_in_this_house_1_year_ago -> categorical<br/>\n",
    "migration_prev_res_in_sunbelt -> categorical, **Not in universe & ?**<br/>\n",
    "family_members_under_18 -> categorical<br/>\n",
    "country_of_birth_father -> categorical<br/>\n",
    "country_of_birth_mother -> categorical<br/>\n",
    "country_of_birth_self -> categorical<br/>\n",
    "citizenship -> categorical<br/>\n",
    "fill_inc_questionnaire_for_veteran's_admin -> categorical<br/>\n",
    "income -> categorical<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_objects_ratios(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_objects_ratios(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'Not in universe', 'Not identifiable' and '?' by null\n",
    "train.replace(\"Not in universe\", np.nan, inplace=True)\n",
    "train.replace(\"Not identifiable\", np.nan, inplace=True)\n",
    "train.replace(\"?\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def missing_data(df):\n",
    "    total = df.isnull().sum()\n",
    "    percent = (df.isnull().sum()/df.shape[0]*100)\n",
    "    missing_values = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        types.append(dtype)\n",
    "    missing_values[\"Types\"] = types\n",
    "    missing_values.sort_values('Total',ascending=False,inplace=True)\n",
    "    return(missing_values)\n",
    "missing_datas = missing_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns with too much missing data:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cols = missing_datas[missing_datas[\"Percent\"] >= 51].index\n",
    "print(empty_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select_dtypes(exclude=[\"object\"]).hist(bins=50, figsize=(20,15))\n",
    "plt.tight_layout(pad=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(train.corr(), cmap=\"Blues\", linewidths=0.75, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sup = train[train[\"income\"] == 1]\n",
    "train_inf = train[train[\"income\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_comparison(col):\n",
    "    train_sup_col = train_sup[col].value_counts()\n",
    "    train_sup_col = pd.DataFrame({col:train_sup_col.index, 'count':train_sup_col.values})\n",
    "\n",
    "    train_inf_col = train_inf[col].value_counts()\n",
    "    train_inf_col = pd.DataFrame({col:train_inf_col.index, 'count':train_inf_col.values})\n",
    "\n",
    "    pie_sup = go.Pie(  \n",
    "       labels = train_sup_col[col],\n",
    "       values = train_sup_col[\"count\"],\n",
    "       domain=dict(x=[0, 0.5]),\n",
    "       name=\"Above 50k\",\n",
    "       hole = 0.5,\n",
    "       marker = dict(colors=['violet', 'cornflowerblue'], line=dict(color='#000000', width=2))\n",
    "    )\n",
    "\n",
    "    pie_inf = go.Pie(  \n",
    "       labels = train_inf_col[col],\n",
    "       values = train_inf_col[\"count\"],\n",
    "       domain=dict(x=[0.5, 1.0]), \n",
    "       name=\"Below 50k\",\n",
    "       hole = 0.5,\n",
    "       marker = dict(colors=['cornflowerblue', 'violet'], line=dict(color='#000000', width=2))\n",
    "    )\n",
    "\n",
    "    data = [pie_sup, pie_inf]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        title=col + ' percentage from +50k vs -50k',\n",
    "        annotations=[dict(text=\"Above 50k\", x=0.18, y=0.5, font_size=15, showarrow=False),\n",
    "                     dict(text=\"Below 50k\", x=0.85, y=0.5, font_size=15, showarrow=False)]\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_comparison(\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_comparison(\"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform(df, empty_cols):\n",
    "    \n",
    "    # Droping empty column\n",
    "    df.drop(empty_cols, axis=1, inplace=True)\n",
    "\n",
    "    # Creating a categorical variable for age\n",
    "    df[\"ageCat\"] = \"\"\n",
    "    df[\"ageCat\"].loc[(df[\"age\"] < 18)] = 'young'\n",
    "    df[\"ageCat\"].loc[(df[\"age\"] >= 18) & (df[\"age\"] < 56)] = 'mature'\n",
    "    df[\"ageCat\"].loc[(df[\"age\"] >= 56)] = 'senior'\n",
    "    \n",
    "    # Creating a categorical variable for hispanic origin\n",
    "    df[\"hispanicCat\"] = 1\n",
    "    df[\"hispanicCat\"].loc[(df[\"hispanic_origin\"] == \"All other\")] = 0\n",
    "    df[\"hispanicCat\"].loc[(df[\"hispanic_origin\"].isna())] = 0\n",
    "    \n",
    "    # Creating a categorical variable to tell if the passenger is a Young/Mature/Senior male or a Young/Mature/Senior female\n",
    "    df[\"SexCat\"] = \"\"\n",
    "    df[\"SexCat\"].loc[(df[\"sex\"] == \"Male\") & (df[\"age\"] <= 21)] = \"youngmale\"\n",
    "    df[\"SexCat\"].loc[(df[\"sex\"] == \"Male\") & ((df[\"age\"] > 21) & (df[\"age\"]) < 50)] = \"maturemale\"\n",
    "    df[\"SexCat\"].loc[(df[\"sex\"] == \"Male\") & (df[\"age\"] > 50)] = \"seniormale\"\n",
    "    df[\"SexCat\"].loc[(df[\"sex\"] == \"Female\") & (df[\"age\"] <= 21)] = \"youngfemale\"\n",
    "    df[\"SexCat\"].loc[(df[\"sex\"] == \"Female\") & ((df[\"age\"] > 21) & (df[\"age\"]) < 50)] = \"maturefemale\"\n",
    "    df[\"SexCat\"].loc[(df[\"sex\"] == \"Female\") & (df[\"age\"] > 50)] = \"seniorfemale\"\n",
    "    \n",
    "    # Dropping unused columns from the feature set\n",
    "    df.drop([\"hispanic_origin\"], axis=1, inplace=True)\n",
    "    \n",
    "    target = df['income']\n",
    "    \n",
    "    # Splitting categorical and numerical column dataframes\n",
    "    categorical_df = df.select_dtypes(include=['object'])\n",
    "    numeric_df = df.select_dtypes(exclude=['object'])\n",
    "    \n",
    "    # And then, storing the names of categorical and numerical columns.\n",
    "    categorical_columns = list(categorical_df.columns)\n",
    "    numeric_columns = list(numeric_df.columns)\n",
    "    \n",
    "    print(\"Categorical columns:\\n\", categorical_columns)\n",
    "    print(\"\\nNumeric columns:\\n\", numeric_columns)\n",
    "\n",
    "    return df, target, categorical_columns, numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, target, categorical_columns, numeric_columns = feature_transform(train, empty_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function responsible for checking our model's performance on the test data\n",
    "def testSetResultsClassifier(classifier, x_test, y_test):\n",
    "    predictions = classifier.predict(x_test)\n",
    "    \n",
    "    results = []\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    results.append(f1)\n",
    "    results.append(precision)\n",
    "    results.append(recall)\n",
    "    results.append(roc_auc)\n",
    "    results.append(accuracy)\n",
    "    \n",
    "    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n",
    "    print(\"F1 score, Precision, Recall, ROC_AUC score, Accuracy:\")\n",
    "    print(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineBestModelPipeline(df, target, categorical_columns, numeric_columns):\n",
    "    \n",
    "    # Splitting original data into Train and Test BEFORE applying transformations\n",
    "    # Later in RandomSearchCV, x_train will be splitted into train/val sets\n",
    "    # The transformations are going to be fitted specifically on the train set,\n",
    "    # and then applied to both train/test sets. This way, information leakage is avoided!\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.10, random_state=42)\n",
    "    y_train = y_train.to_numpy() # Transforming training targets into numpy arrays\n",
    "    y_test = y_test.to_numpy() # Transforming test targets into numpy arrays\n",
    "    \n",
    "    \n",
    "    # # If desired, we can balance training classes using one of the functions below\n",
    "    # # Obtaining balanced data for modeling using Random Under Sampling\n",
    "    #x_train, y_train = balancingClassesRus(x_train, y_train)\n",
    "\n",
    "    # # Obtaining balanced data for modeling using SMOTEENN\n",
    "    #x_train, y_train = balancingClassesSmoteenn(x_train, y_train)\n",
    "\n",
    "    # # Obtaining balanced data for modeling using SMOTE\n",
    "    #x_train, y_train = balancingClassesSmote(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    # 1st -> Numeric Transformers\n",
    "    # Here, we are creating different several different data transformation pipelines \n",
    "    # to be applied in our numeric features\n",
    "    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),\n",
    "                                            ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),\n",
    "                                            ('scaler', StandardScaler())])\n",
    "    \n",
    "    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n",
    "                                            ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n",
    "                                            ('scaler', StandardScaler())])\n",
    "    \n",
    "    \n",
    "    # 2nd -> Categorical Transformer\n",
    "    # Despite my option of not doing it, you can also choose to create different \n",
    "    # data transformation pipelines for your categorical features.\n",
    "    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n",
    "                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n",
    "    \n",
    "    \n",
    "    # 3rd -> Combining both numerical and categorical pipelines\n",
    "    # Here, we are creating different ColumnTransformers, each one with a different numerical transformation\n",
    "    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_columns),\n",
    "                                                             ('cat', categorical_transformer, categorical_columns)])\n",
    "    \n",
    "    \n",
    "    # And finally, we are going to apply these different data transformations to RandomSearchCV,\n",
    "    # trying to find the best imputing strategy, the best feature engineering strategy\n",
    "    # and the best model with it's respective parameters.\n",
    "    # Below, we just need to initialize a Pipeline object with any transformations we want, on each of the steps.\n",
    "    pipe = Pipeline(steps=[('data_transformations', data_transformations_1), # Initializing data transformation step by choosing any of the above\n",
    "                           ('feature_eng', PCA()), # Initializing feature engineering step by choosing any desired method\n",
    "                           ('clf', SVC())]) # Initializing modeling step of the pipeline with any model object\n",
    "                           #memory='cache_folder') -> Used to optimize memory when needed\n",
    "    \n",
    "    \n",
    "    # Now, we define the grid of parameters that RandomSearchCV will use. It will randomly chose\n",
    "    # options for each step inside the dictionaries ('data transformations', 'feature_eng', 'clf'\n",
    "    # and 'clf parameters'). In the end of it's iterations, RandomSearchCV will return the best options.\n",
    "    params_grid = [\n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [KNeighborsClassifier()],\n",
    "                     'clf__n_neighbors': stats.randint(1, 50),\n",
    "                     'clf__metric': ['minkowski', 'euclidean']},\n",
    "\n",
    "        \n",
    "\n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [LogisticRegression()],\n",
    "                     'clf__penalty': ['l1', 'l2'],\n",
    "                     'clf__C': stats.uniform(0.01, 10)},\n",
    "\n",
    "\n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [SVC()],\n",
    "                     'clf__C': stats.uniform(0.1, 10),\n",
    "                     'clf__gamma': stats.uniform(0.1, 10)},\n",
    "\n",
    "\n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [DecisionTreeClassifier()],\n",
    "                     'clf__criterion': ['gini', 'entropy'],\n",
    "                     'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "                     'clf__max_depth': [None, stats.randint(1, 5)]},\n",
    "\n",
    "\n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [RandomForestClassifier()],\n",
    "                     'clf__n_estimators': stats.randint(10, 175),\n",
    "                     'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "                     'clf__max_depth': [None, stats.randint(1, 5)],\n",
    "                     'clf__random_state': stats.randint(1, 49)},\n",
    "        \n",
    "                    \n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [ExtraTreesClassifier()],\n",
    "                     'clf__n_estimators': stats.randint(10, 150),\n",
    "                     'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "                     'clf__max_depth': [None, stats.randint(1, 6)]},\n",
    "\n",
    "                    \n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [GradientBoostingClassifier()],\n",
    "                     'clf__n_estimators': stats.randint(10, 100),\n",
    "                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n",
    "                     'clf__max_depth': [None, stats.randint(1, 6)]},\n",
    "\n",
    "        \n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [LGBMClassifier()],\n",
    "                     'clf__n_estimators': stats.randint(1, 100),\n",
    "                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n",
    "                     'clf__max_depth': [None, stats.randint(1, 6)]},\n",
    "\n",
    "\n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [XGBClassifier()],\n",
    "                     'clf__n_estimators': stats.randint(5, 125),\n",
    "                     'clf__eta': stats.uniform(0.01, 1),\n",
    "                     'clf__max_depth': [None, stats.randint(1, 6)],\n",
    "                     'clf__gamma': stats.uniform(0.01, 1)},\n",
    "\n",
    "\n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [StackingClassifier(estimators=[('svc', SVC(C=1, gamma=1)),\n",
    "                                                            ('rf', RandomForestClassifier(max_depth=5, n_estimators=50, n_jobs=-1, random_state=28)),\n",
    "                                                            ('xgb', XGBClassifier(eta=0.6, gamma=0.7, max_depth=None, n_estimators=30))], \n",
    "                                                final_estimator=LogisticRegression(C=1))]},\n",
    "   \n",
    "   \n",
    "        \n",
    "                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n",
    "                     'feature_eng': [None, \n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n",
    "                     'clf': [VotingClassifier(estimators=[('gbt', GradientBoostingClassifier(learning_rate=0.8, max_depth=None, n_estimators=30)),\n",
    "                                                          ('lgbm', LGBMClassifier(n_estimators=30, learning_rate=0.6, max_depth=None)),\n",
    "                                                          ('xgb', XGBClassifier(eta=0.8, gamma=0.8, max_depth=None, n_estimators=40))],\n",
    "                                              voting='soft')]}\n",
    "                ]\n",
    "    \n",
    "    \n",
    "    # Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    # Creating our cross validation object with StratifiedShuffleSplit, 10 folds\n",
    "    # Stratification assures that we split the data such that the proportions\n",
    "    # between classes are the same in each fold as they are in the whole dataset\n",
    "    cross_validator = StratifiedShuffleSplit(n_splits=10, train_size=0.8, test_size=0.2, random_state=7)\n",
    "    \n",
    "    # Creating the randomized search cv object and fitting it\n",
    "    best_model_pipeline = RandomizedSearchCV(estimator=pipe, param_distributions=params_grid, \n",
    "                                             n_iter=100, scoring=metrics, refit='accuracy', \n",
    "                                             n_jobs=-1, cv=cross_validator, random_state=21)\n",
    "\n",
    "    best_model_pipeline.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    # At last, we check the final results\n",
    "    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n",
    "    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n",
    "    print(\"\\n\\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n",
    "    print(\"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, best_model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
